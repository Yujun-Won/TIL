# Aivle School 머신러닝 베스트 질문

- [1. 다중회귀 모델에서의 회귀식](#1-다중회귀-모델에서의-회귀식)
- [2. R²와 RMSE, MAP, MAPE](#2-r²와-rmse-map-mape)
- [3. KNeighborsClassifier, KNeighborsRegressor 두 개의 사용 차이](#3-kneighborsclassifier-kneighborsregressor-두-개의-사용-차이)
- [4. 쿼리문 작성할 때 대문자와 소문자](#4-쿼리문-작성할-때-대문자와-소문자)
- [5. Precison과 Recall의 관계](#5-precision과-recall의-관계)
- [6. Information Gain](#6-information-gain)
- [7. K-fold와 Stratified K-fold](#7-k-fold와-stratified-k-fold)
- [6. Information Gain](#6-information-gain)



## 1. 다중회귀 모델에서의 회귀식
```plain text
다중회귀 모델에서 feature가 2가지로 가정한 경우의 선형 회귀식은 아래와 같다.
```
$$y=w_0 + w_1 * x_1 + w_2 * x_2$$

- $w_0$: 고유의 선형계수
- $w_1$, $w_2$: 회귀계수


## 2. R²와 RMSE, MAP, MAPE
```plain text
RMSE, MAP, MAPE는 값이 작을수록 회귀 성능이 좋은 것으로 해석된다.
R²는 값이 클수록(1에 가까울수록) 회귀 성능이 좋은 것으로 해석된다.

회귀 평가 지표의 경우, 모델이 아니기 때문에 더 나은 모델을 고르는 것보다 평가 지표로써 활용된다.
```

### (1) R² (R-Squared, 결정 계수)
- **설명**
    ```plain text
    R²는 회귀 모델의 설명력을 나타내는 지표. 즉, 모델이 얼마나 데이터를 잘 설명하는지를 나타낸다.
    값은 0과 1 사이로, 1에 가까울수록 모델이 데이터를 잘 설명한다는 것을 의미합니다.
    ```

- **수식**

  ![texclip20230903112720](https://github.com/Yujun-Won/TIL/assets/124374862/4eca61bb-f294-4635-bb81-fa9d3d5ecac5)

  여기서 $y_i$는 실제 값, $\hat{y}_i$는 예측 값, $\bar{y}$는 실제 값의 평균입니다.

### (2) RMSE (Root Mean Square Error)
- **설명**
    ```plain text
    RMSE는 회귀 모델의 예측 오차를 나타내는 지표이다.
    값이 작을수록 모델의 예측 성능이 좋다는 것을 의미한다.
    ```
- **수식**
  $$RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}$$
  여기서 $y_i$는 실제 값, $\hat{y}_i$는 예측 값입니다.

### (3) MAP (Mean Absolute Percentage)
- **설명**
    ```plain text
    MAP는 분류 문제에서 모델의 성능을 평가하는 데 사용되는 지표이다.
    각 클래스에 대한 정확도의 평균을 계산한다.
    ```
- **수식**
  $$MAP = \frac{1}{Q} \sum_{q=1}^{Q} \frac{1}{M} \sum_{m=1}^{M} P(m)$$
  여기서 $Q$는 쿼리 수, $M$은 각 쿼리에 대한 문서 수, $P(m)$은 $m$번째 문서까지의 정밀도입니다.

### (4) MAPE (Mean Absolute Percentage Error)
- **설명**
    ```plain text
    MAPE는 회귀 모델의 예측 오차를 백분율로 나타내는 지표이다.
    값이 작을수록 모델의 예측 성능이 좋다는 것을 의미한다.
    ```
- **수식**:
  $$MAPE = \frac{100}{n} \sum_{i=1}^{n} \left| \frac{y_i - \hat{y}_i}{y_i} \right|$$
  여기서 $y_i$는 실제 값, $\hat{y}_i$는 예측 값입니다.

## 3. KNeighborsClassifier, KNeighborsRegressor 두 개의 사용 차이
```plain text
KNeighborsClassifier는 분류모델에서 사용하고, KNeighborsRegressor는 회귀모델에서 사용한다.

일반적으로 KNN은 분류에 사용하는 알고리즘이지만, 이를 활용해서 주변 값들에 대한 가중평균을 계산하고 그 값을 예측하는 방식으로 KNeighborsRegressor 모델을 사용할 수 있다.
```

## 4. 가설 검정을 하는 이유
```plain
가설 검정을 하는 이유는 모델링할 때 타겟과 관련 있는 feature를 선택하기 위함이다.

가설을 설정하고 설정한 가설들을 바탕으로 어느 정도로 연관이 있는지 확인하기 위한 방법으로 가설 검정은 효과적이다.
특히 통계적 비즈니스 모델링, 데이터 분석이 대부분 데이터의 feature 간 연관성이나 상관성을 바탕으로 진행된다는 점에서 연관이 있다는 점을 p-value까지 계산하여 제공하는 것은 매우 강력한 분석방법이라고 할 수 있다.

하지만, 가설 검정 자체는 통계에서 사용하는 추론 방법론 중 하나에 불과하다.
즉, 가설이 맞을 가능성을 확률론적으로 풀어내고, 그것을 이용해 추론해내는 방법론이기 때문에 절대적인 방법은 아니다.
```

## 5. Precision과 Recall의 관계
- **정밀도(Precision)**: $\text{Precision} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Positive}}$

- **재현율(Recall)**: $\text{Recall} = \frac{\text{True Positive}}{\text{True Positive} + \text{False Negative}}$

    - True Positive: 실제로 1이면서 예측도 1인 경우
    - False Positive: 실제로 0인데 예측이 1인 경우
    - False Negative: 실제로 1인데 예측이 0인 경우

```plain text
Precision과 Recall은 서로 Trade-off 관계에 있다.

Precision을 높이다가 Recall을 떨어뜨리는 경우는 아래와 같다.
이 경우, 모델이 매우 보수적으로 예측을 하는 상황이다.
즉, 모델이 1이라고 예측하는 경우는 그것이 정말로 확실한 1일 때!

(예시) 스팸 메일 필터
- Precision 높음: 필터는 오직 100% 확실한 스팸 메일만 스팸으로 분류
- Recall 낮음: 보수적인 접근법 때문에 일부 스팸 메일은 스팸으로 분류되지 않고 빠질 수 있음

정밀도를 높이려면 False Positive를 줄여야 하고, 이는 자연스럽게 True Positive도 줄어들게 만들어 재현율을 떨어뜨립니다.
```

## 6. Information Gain
```plain text
정보 이득(Information Gain)은 엔트로피 기반 개념

엔트로피는 주어진 데이터의 혼잡도 의미한다.
데이터에 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 낮다.

정보 이득은 특정 변수를 사용했을 때 엔트로피의 감소량으로써 어떤 노드를 선택하는 것이 옳은지 따져볼 때 사용하는 기댓값이다.

정보이득 지수는 "1-엔트로피"
```

## 7. K-fold와 Stratified K-fold
```plain text
StratifiedKFold는 클래스의 비율을 고려하여 샘플을 추출한다.
따라서, K-Fold에 균일한 비율을 유지한 채로 만들어진다.

stratified 옵션은 train_test_split()에서도 적용이 가능하다.
만일 스플릿 과정에서 stratified를 적용했다면 굳이 stratifiedKFold을 사용하지 않아도 된다.
```

## 8. random_state 옵션
```plain text
배운 알고리즘 중 random_state 옵션이 있는 것들은 다음과 같다.
LogisticRegression, DecisionTree, RandomForest, XGB

각 알고리즘마다 학습시 내부에서 랜덤하게 처리되는 부분이 있다.
- LogisticRegression : solver == ‘sag’, ‘saga’ or ‘liblinear’ 일 때 데이터를 랜덤하게 섞음

- Decision Tree
    1. max_features 값이 지정된 경우, feature를 랜덤하게 선택
    2. 연속형 변수를 split할 때 무작위 작업이 포함

- RandomForest : bootstrap과 feature 선택

- XGB : bootstrap 샘플링

- SVC : probability(확률추정)을 True로 설정할 때, 데이터를 랜덤 셔플링
```

## 9. warm_start 옵션
```plain text
warm_start=True로 하고 학습하면, 기존 모델은 그대로 유지한 채 tree를 추가하여 모델을 생성한다.

만약 랜덤포레스트 tree 100개 짜리를 모델(m1)을 만든 후, tree 10개를 더 추가하고 싶어서 warm_start=True로 설정하면, m1의 트리 100 + 새로 학습한 트리 10개가 하나의 랜덤포레스트 모델이 된다.
```
